{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef80fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_7904\\1507856525.py:4: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/insurance_data.csv\", sep='|')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnderwrittenCoverID               0\n",
      "PolicyID                          0\n",
      "TransactionMonth                  0\n",
      "IsVATRegistered                   0\n",
      "Citizenship                       0\n",
      "LegalType                         0\n",
      "Title                             0\n",
      "Language                          0\n",
      "Bank                         145961\n",
      "AccountType                   40232\n",
      "MaritalStatus                  8259\n",
      "Gender                         9536\n",
      "Country                           0\n",
      "Province                          0\n",
      "PostalCode                        0\n",
      "MainCrestaZone                    0\n",
      "SubCrestaZone                     0\n",
      "ItemType                          0\n",
      "mmcode                          552\n",
      "VehicleType                     552\n",
      "RegistrationYear                  0\n",
      "make                            552\n",
      "Model                           552\n",
      "Cylinders                       552\n",
      "cubiccapacity                   552\n",
      "kilowatts                       552\n",
      "bodytype                        552\n",
      "NumberOfDoors                   552\n",
      "VehicleIntroDate                552\n",
      "CustomValueEstimate          779642\n",
      "AlarmImmobiliser                  0\n",
      "TrackingDevice                    0\n",
      "CapitalOutstanding                2\n",
      "NewVehicle                   153295\n",
      "WrittenOff                   641901\n",
      "Rebuilt                      641901\n",
      "Converted                    641901\n",
      "CrossBorder                  999400\n",
      "NumberOfVehiclesInFleet     1000098\n",
      "SumInsured                        0\n",
      "TermFrequency                     0\n",
      "CalculatedPremiumPerTerm          0\n",
      "ExcessSelected                    0\n",
      "CoverCategory                     0\n",
      "CoverType                         0\n",
      "CoverGroup                        0\n",
      "Section                           0\n",
      "Product                           0\n",
      "StatutoryClass                    0\n",
      "StatutoryRiskType                 0\n",
      "TotalPremium                      0\n",
      "TotalClaims                       0\n",
      "dtype: int64\n",
      "UnderwrittenCoverID               0\n",
      "PolicyID                          0\n",
      "TransactionMonth                  0\n",
      "IsVATRegistered                   0\n",
      "Citizenship                       0\n",
      "LegalType                         0\n",
      "Title                             0\n",
      "Language                          0\n",
      "Bank                         145961\n",
      "AccountType                   40232\n",
      "MaritalStatus                  8259\n",
      "Gender                            0\n",
      "Country                           0\n",
      "Province                          0\n",
      "PostalCode                        0\n",
      "MainCrestaZone                    0\n",
      "SubCrestaZone                     0\n",
      "ItemType                          0\n",
      "mmcode                          552\n",
      "VehicleType                     552\n",
      "RegistrationYear                  0\n",
      "make                            552\n",
      "Model                           552\n",
      "Cylinders                       552\n",
      "cubiccapacity                   552\n",
      "kilowatts                       552\n",
      "bodytype                        552\n",
      "NumberOfDoors                   552\n",
      "VehicleIntroDate                552\n",
      "CustomValueEstimate          779642\n",
      "AlarmImmobiliser                  0\n",
      "TrackingDevice                    0\n",
      "CapitalOutstanding                2\n",
      "NewVehicle                   153295\n",
      "WrittenOff                   641901\n",
      "Rebuilt                      641901\n",
      "Converted                    641901\n",
      "CrossBorder                  999400\n",
      "NumberOfVehiclesInFleet     1000098\n",
      "SumInsured                        0\n",
      "TermFrequency                     0\n",
      "CalculatedPremiumPerTerm          0\n",
      "ExcessSelected                    0\n",
      "CoverCategory                     0\n",
      "CoverType                         0\n",
      "CoverGroup                        0\n",
      "Section                           0\n",
      "Product                           0\n",
      "StatutoryClass                    0\n",
      "StatutoryRiskType                 0\n",
      "TotalPremium                      0\n",
      "TotalClaims                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/insurance_data.csv\", sep='|')\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Example: Remove rows where 'TotalClaims' is missing\n",
    "df = df.dropna(subset=['TotalClaims'])\n",
    "\n",
    "# Or Impute: Example for numerical features using mean\n",
    "df['TotalClaims'] = df['TotalClaims'].fillna(df['TotalClaims'].mean())\n",
    "\n",
    "# For categorical features, use mode (most frequent value) imputation\n",
    "df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])\n",
    "\n",
    "# Confirm no missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3795ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalPolicies'] = 1\n",
    "df['ClaimFreq'] = df['TotalClaims'] / df['TotalPolicies']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "df['ClaimFreq'] = df['TotalClaims'] / df['TotalPolicies']  # if these columns exist\n",
    "df['PremiumToClaimsRatio'] = df['TotalPremium'] / df['TotalClaims']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a16289cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for categorical variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# If using label encoding for a binary column like 'Gender':\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#label_encoder = LabelEncoder()\n",
    "#df['Gender_Male'] = label_encoder.fit_transform(df['Gender_Male'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "905e634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['TotalClaims', 'CalculatedPremiumPerTerm'])\n",
    "y = df['TotalClaims']  # or 'ClaimOccurrence' for the binary classification model\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338850c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HasClaim column (if not already created)\n",
    "df['HasClaim'] = (df['TotalClaims'] > 0).astype(int)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['Gender_Male', 'Gender_Not specified']].fillna(0)\n",
    "y = df['HasClaim']\n",
    "\n",
    "# Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f68b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7090e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00278351 0.00278351 0.00278351 0.00278351 0.00278351]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select features and target\n",
    "X = df[['Gender_Male', 'Gender_Not specified']].fillna(0)\n",
    "y = df['HasClaim']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to make duplicate columns unique by adding suffixes\n",
    "def make_cols_unique(cols):\n",
    "    seen = {}\n",
    "    result = []\n",
    "    for col in cols:\n",
    "        if col not in seen:\n",
    "            seen[col] = 0\n",
    "            result.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            result.append(f\"{col}_{seen[col]}\")\n",
    "    return result\n",
    "\n",
    "# Fix duplicate columns in train and test sets\n",
    "X_train.columns = make_cols_unique(X_train.columns)\n",
    "X_test.columns = make_cols_unique(X_test.columns)\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters and train model\n",
    "params = {'objective': 'reg:squarederror'}\n",
    "xg_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict\n",
    "y_pred_xg = xg_model.predict(dtest)\n",
    "\n",
    "print(y_pred_xg[:5])  # print first 5 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf300ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fit model to predict claim occurrence (binary)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict claim occurrence (0 or 1)\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d71ace73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - RMSE: 0.05381683765911473, R-squared: 6.9661862265313346e-06\n",
      "Random Forest - RMSE: 0.053816863418747495, R-squared: 6.008885106489892e-06\n",
      "XGBoost - RMSE: 0.05381683833382197, R-squared: 7.033348083496094e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# RMSE for Linear Regression\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# RMSE for Random Forest\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# RMSE for XGBoost\n",
    "rmse_xg = np.sqrt(mean_squared_error(y_test, y_pred_xg))\n",
    "r2_xg = r2_score(y_test, y_pred_xg)\n",
    "\n",
    "# Print Evaluation Results\n",
    "print(f\"Linear Regression - RMSE: {rmse_lr}, R-squared: {r2_lr}\")\n",
    "print(f\"Random Forest - RMSE: {rmse_rf}, R-squared: {r2_rf}\")\n",
    "print(f\"XGBoost - RMSE: {rmse_xg}, R-squared: {r2_xg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec4649cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.9970952904709529, F1-score: 0.0, Precision: 0.0, Recall: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Accuracy, F1-Score, Precision, and Recall for Logistic Regression\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "f1_logreg = f1_score(y_test, y_pred_logreg)\n",
    "precision_logreg = precision_score(y_test, y_pred_logreg)\n",
    "recall_logreg = recall_score(y_test, y_pred_logreg)\n",
    "\n",
    "# Print Classification Results\n",
    "print(f\"Logistic Regression - Accuracy: {accuracy_logreg}, F1-score: {f1_logreg}, Precision: {precision_logreg}, Recall: {recall_logreg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e57735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'TransactionMonth' to datetime format\n",
    "df['TransactionMonth'] = pd.to_datetime(df['TransactionMonth'], errors='coerce')\n",
    "\n",
    "# Create a new feature: months since the earliest transaction\n",
    "df['MonthsSinceStart'] = (df['TransactionMonth'] - df['TransactionMonth'].min()).dt.days // 30\n",
    "\n",
    "# Drop the original 'TransactionMonth' column if it's no longer needed\n",
    "df = df.drop(columns=['TransactionMonth'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "788fdabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UnderwrittenCoverID', 'PolicyID', 'IsVATRegistered', 'AccountType',\n",
      "       'MaritalStatus', 'Gender', 'PostalCode', 'mmcode', 'VehicleType',\n",
      "       'RegistrationYear',\n",
      "       ...\n",
      "       'Model_VOLARE W8 28 SEAT B/S B/S', 'Model_X TRAIL 2.0 4X2 XE (R79/R85)',\n",
      "       'Model_YARIS 1.3 5Dr', 'Model_YARIS T1 3Dr', 'Model_YARIS T1 5Dr',\n",
      "       'Model_YARIS T3 A/C', 'Model_YARIS T3 SPIRIT 5Dr', 'Model_YARIS T3+',\n",
      "       'Model_YARIS ZEN3', 'Model_YARIS ZEN3 ACS'],\n",
      "      dtype='object', length=585)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# One-hot encode categorical variables with multiple categories\n",
    "df = pd.get_dummies(df, columns=['Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'Country', 'Province', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'make', 'Model'], drop_first=True)\n",
    "\n",
    "# Label encode binary categorical columns like 'Gender' (Male/Female)\n",
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "\n",
    "# Verify the columns after encoding\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e853b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: drop the target variable and non-useful columns (like IDs, etc.)\n",
    "X = df.drop(columns=['TotalClaims', 'PolicyID', 'UnderwrittenCoverID'])\n",
    "\n",
    "# Target: TotalClaims\n",
    "y = df['TotalClaims']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "764f41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800078, Test set size: 200020\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify split\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c939dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UnderwrittenCoverID  PolicyID  IsVATRegistered  Gender  PostalCode  \\\n",
      "0               145249     12827             True       2        1459   \n",
      "1               145249     12827             True       2        1459   \n",
      "2               145249     12827             True       2        1459   \n",
      "3               145255     12827             True       2        1459   \n",
      "4               145255     12827             True       2        1459   \n",
      "\n",
      "       mmcode  RegistrationYear  Cylinders  cubiccapacity  kilowatts  ...  \\\n",
      "0  44069150.0              2004        6.0         2597.0      130.0  ...   \n",
      "1  44069150.0              2004        6.0         2597.0      130.0  ...   \n",
      "2  44069150.0              2004        6.0         2597.0      130.0  ...   \n",
      "3  44069150.0              2004        6.0         2597.0      130.0  ...   \n",
      "4  44069150.0              2004        6.0         2597.0      130.0  ...   \n",
      "\n",
      "  Model_YARIS ZEN3  Model_YARIS ZEN3 ACS AccountType_Savings account  \\\n",
      "0            False                 False                       False   \n",
      "1            False                 False                       False   \n",
      "2            False                 False                       False   \n",
      "3            False                 False                       False   \n",
      "4            False                 False                       False   \n",
      "\n",
      "   AccountType_Transmission account MaritalStatus_Not specified  \\\n",
      "0                             False                        True   \n",
      "1                             False                        True   \n",
      "2                             False                        True   \n",
      "3                             False                        True   \n",
      "4                             False                        True   \n",
      "\n",
      "  MaritalStatus_Single VehicleType_Heavy Commercial  \\\n",
      "0                False                        False   \n",
      "1                False                        False   \n",
      "2                False                        False   \n",
      "3                False                        False   \n",
      "4                False                        False   \n",
      "\n",
      "  VehicleType_Light Commercial VehicleType_Medium Commercial  \\\n",
      "0                        False                         False   \n",
      "1                        False                         False   \n",
      "2                        False                         False   \n",
      "3                        False                         False   \n",
      "4                        False                         False   \n",
      "\n",
      "  VehicleType_Passenger Vehicle  \n",
      "0                          True  \n",
      "1                          True  \n",
      "2                          True  \n",
      "3                          True  \n",
      "4                          True  \n",
      "\n",
      "[5 rows x 590 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have loaded your dataset into `df`\n",
    "# List the categorical columns you need to one-hot encode\n",
    "categorical_columns = ['AccountType', 'MaritalStatus', 'VehicleType']\n",
    "\n",
    "# Apply One-Hot Encoding to the categorical columns\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Check the resulting dataframe to verify encoding\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59295dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnderwrittenCoverID              0\n",
      "PolicyID                         0\n",
      "IsVATRegistered                  0\n",
      "Gender                           0\n",
      "PostalCode                       0\n",
      "                                ..\n",
      "MaritalStatus_Single             0\n",
      "VehicleType_Heavy Commercial     0\n",
      "VehicleType_Light Commercial     0\n",
      "VehicleType_Medium Commercial    0\n",
      "VehicleType_Passenger Vehicle    0\n",
      "Length: 590, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with appropriate strategies (mean, median, or mode)\n",
    "df['mmcode'] = df['mmcode'].fillna(df['mmcode'].median())  # For numeric columns\n",
    "df['PostalCode'] = df['PostalCode'].fillna(df['PostalCode'].mode()[0])  # For categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26761c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['TotalClaims', 'UnderwrittenCoverID', 'PolicyID'])  # Drop unnecessary columns\n",
    "y = df['TotalClaims']  # Target: TotalClaims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83c4eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800078, Test set size: 200020\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (80% for training, 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the split\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a9e0e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'B/S'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m rf_model = RandomForestRegressor(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[32m     11\u001b[39m y_pred_rf = rf_model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:359\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    372\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:971\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    970\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    973\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:6662\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6656\u001b[39m     results = [\n\u001b[32m   6657\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6658\u001b[39m     ]\n\u001b[32m   6660\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6661\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6662\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6663\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\kifiya\\insurance-risk-analysis\\venv\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'B/S'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model (RMSE and R-squared)\n",
    "rmse = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
